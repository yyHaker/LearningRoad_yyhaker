# Seq2Seq learning resource

[1] [《neural machine translation by jointly learning to align and translate》(2015ICLR)paper阅读笔记七](https://zhuanlan.zhihu.com/p/33192167)

[2] [Seq2seq for French to English translation(有attention)](https://zhuanlan.zhihu.com/p/30963256) 有attention is all your need实现

[3] [pytorch nmt的教程](http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

[4] [OpenNMT官方教程](http://opennmt.net/OpenNMT-py/) [OpenNMT-py github](https://github.com/OpenNMT/OpenNMT-py)  [OpenNMT论坛](http://forum.opennmt.net/)

[5][attention is all your need 论文讲解和代码](https://zhuanlan.zhihu.com/p/33469791)

[6][模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用](https://zhuanlan.zhihu.com/p/31547842)

[7][自然语言处理中的自注意力机制（Self-Attention Mechanism）](https://zhuanlan.zhihu.com/p/35041012?group_id=962364368390934528)

[8][深度学习中的注意力机制2017](https://mp.weixin.qq.com/s?__biz=MzA4Mzc0NjkwNA==&mid=2650783542&idx=1&sn=3846652d54d48e315e31b59507e34e9e&chksm=87fad601b08d5f17f41b27bb21829ed2c2e511cf2049ba6f5c7244c6e4e1bd7144715faa8f67&mpshare=1&scene=1&srcid=1113JZIMxK3XhM9ViyBbYR76#rd)
    (这个是我目前见过最好的介绍seq2seq以及attention机制的文章)
    
[9][tensorflow seq2seq->nmt tutorial(非常详细)](https://github.com/tensorflow/nmt)